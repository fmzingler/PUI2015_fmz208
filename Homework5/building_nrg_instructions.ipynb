{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pylab as pl\n",
    "\n",
    "%pylab inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using geopanda. that is **not required** for this particular exercise, but geopand works with geospacial data: the shape files that we get from pluto for example.\n",
    "\n",
    "PLEASE REMEMBER: seed your random functions if you need to use any, label your axes clearly, comment your code, use PEP8!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##We will use 2 different datasets.\n",
    "##an interesting urban question is \"can we measure and predict energy use based on observables that are easier to be acquired\". for example the urban observatory at CUSP can monitor lights: they are a realtively easy observable. All you need is a camera, and a pipeline to process your data. But how does the light coming from a window relate to the total energy consumption? We generally postulate that light is a proxy for occupancy, and that occupancy is a good predictor of energy consumption.\n",
    "##so let's test if the last link holds. If we have data on the energy consumed by a __building__ how well does that relate to the number of units in the building?\n",
    "\n",
    "## data on energy consumption can be found here for the city of NY https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/rgfe-8y2z  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-934fc1e98b77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnrg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnrg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1025\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3163)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:5779)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv does not exist"
     ]
    }
   ],
   "source": [
    "nrg = gp.GeoDataFrame.from_csv(\"Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv\")\n",
    "print nrg.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#however this datasets does not have the number of units. We can find that in the Pluto dataset.\n",
    "reading in the Pluto data for manhattan, which will give me the number of units ber building   Manhattan/MNMapPLUTO.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "no such file or directory: 'Manhattan/MNMapPLUTO.shp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b9558afb69ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##remove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Manhattan/MNMapPLUTO.shp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mbsize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\geopandas-0.1.0.dev_bdfc7fb-py2.7.egg\\geopandas\\geodataframe.pyc\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \"\"\"\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\geopandas-0.1.0.dev_bdfc7fb-py2.7.egg\\geopandas\\io\\file.pyc\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \"\"\"\n\u001b[0;32m     19\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bbox'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Fernando\\Anaconda\\lib\\site-packages\\fiona\\__init__.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no such archive file: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         c = Collection(path, mode, driver=driver,\n\u001b[0;32m    171\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvsi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: no such file or directory: 'Manhattan/MNMapPLUTO.shp'"
     ]
    }
   ],
   "source": [
    "##remove\n",
    "\n",
    "bsize = gp.GeoDataFrame.from_file(\"Manhattan/MNMapPLUTO.shp\")\n",
    "print bsize.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in pandas scatter_matrix will plot each column agains each other. that is a great way to begin inspecting your data (though be careful cause it is computationally comsuming for large datasets) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nrg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-01ba79063f74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscatter_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscatter_matrix\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kde'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nrg' is not defined"
     ]
    }
   ],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix (nrg, s=300, figsize=(16, 16), diagonal='kde')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it will show me correlations between variables, but it will also show me what can and \n",
    "cannot be plotted trivially in my dataset! here only a few columns can be plotted: \n",
    "those that contain only numbers (and possibly NaN's), but most columns contain rows that cannot be converted to float (e.g. entries like 'See Primary BBL' in several rows for the energy dataframe 'Site EUI(kBtu/ft2) ' column) , so Pandas refuses to plot them, cause it does not know what to do with those entries. \n",
    "The columns I am interested in are in fact u'Site EUI(kBtu/ft2)' which is a measure of the energy consumed PER SQ FOOT by a building, and then the building area: for eg.       u'Reported Property Floor Area (Building(s)) (ft²)'. Neither gets plotted: i need to remove all values that cannot conver to float in order to use the columns and convert them to float arrays and do math with them. \n",
    "\n",
    "Below I created a function that returns True if what is passed as argument can be converted to a float, and False otherwise. I do it with the Try/Except structure. it is a very useful python functionality that you should look into.  \n",
    "this has to be done entry by entry: if you gve t a series you get error \"cannot convert the series to <type 'float'>\"\n",
    "\n",
    "However you can pass it a whole array by using it in conjunction with\n",
    "np.vectorize (look up the syntax!) which will implement a for loop for you under the hood to pass the series entry by entry to your function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def canconvert(mydata):\n",
    "    try :\n",
    "        float(mydata)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for every column i care about i can create an array, call it for eg. indx, of True and False (a Boolean array) that i can use as an index to pick entries that \n",
    "I want to use from my arrays of interest. I can replace the entries that failed to be converted to 'NaN'\n",
    "\n",
    "(because my function return Trues when I can convert and False otherwise i actuallly want the complementary array: ~indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the indeces array here\n",
    "#indx = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrg['Site EUI(kBtu/ft2)'][~indx]=float('NaN')\n",
    "\n",
    "\n",
    "##etc etc... for all columns you care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##check that your conversion worked: e.g.\n",
    "print nrg['Site EUI(kBtu/ft2)'].astype(float)\n",
    "#[...] check all columns you care about in both datasets. \n",
    "#Nan's are ok, but you must not get an error when you try the conversion\n",
    "#the Pluto data is much better at not using spurious entries for numerical value columns. \n",
    "#but check you can use the columns you want\n",
    "print bsize.BBL.astype(float)\n",
    "#notice i can use the attribute notation to refer to columns in bsize, \n",
    "#cause the column names do not have spaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TO MERGE THE DATASET WE NEED A COMMON COLUMN\n",
    "here is where you want to merge the datasets. You need a common column: the building id BBL\n",
    "However the name of this variable (column) in the Energy dataset is     'NYC Borough, Block, and Lot (BBL)'. create a column 'BBL' in the nrg dataset to pass it to the 'on' keyword argument of the merge pandas method: pd.merge(..... on=['BBL']) will use the common column 'BBL' to join the information from the 2 datasets for each BBL value (check out the complete syntax!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nrg['BBL'] = ...\n",
    "#bblnrgdata = pd.merge(.....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once you have the dataframe with all the info you want, you want to plot \n",
    "Energy vs Number of Units in the Building. \n",
    "Remember: Energy TOTAL, not per sq ft...\n",
    "Here you can choose what you think makes more sense for the number of units: all units, residential units...\n",
    "make a 'natural' scale scatter plot first it will look really bad cause all the datapoints are at very low Unit numbers while the Unit number range actually goes up to 8000.\n",
    "perhaps zoom into the region where most points are by cutting your x and y axis plotted: e,g, use xlim=(1000,1e10), ylim=(1,1000), where the numbers to be plugged in depend on exactly what you chose to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when the points in a plot all look scrunched against the axis try a log plot! in pandas you enable that with the keyword 'loglog' :\n",
    "bblnrgdata.plot(..... loglog=True)\n",
    "\n",
    "NOTICE THAT YOU WILL STILL HAVE TO CUT YOUR DATASET! in my data i had a lot of energy datapoints that were exactly 0. I removed these \"outliers\" which I think are truly outliers in the sense that they are misreported numbers. you can remove the data that have nrg==0 (or nrg < some sensible threshold choice) by indexing your array:\n",
    "something like bblnrgdata_cut = bblnrgdata[bblnrgdata.nrg>1000]\n",
    "\n",
    "also i removed the buildings with several thousand units. points like that at the edge of my range would have a lot of \"LEVERAGE\", however  they are not suprious entries like the 0, which i believe are missing values, or perhaps abandoned lots. these are good datapoint that i need to throw away functionally for my analysis to work, but that should be stated clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a line through the data. you can use whatever you want to do it: \n",
    "statsmodels, scipy, any other package, or write your own minimization of the residuals\n",
    "## BUT REMEMBER: we see hints of a lienar relation in log space! so we want to fit a line to the log of our variables, not the variables themselves:\n",
    "\n",
    "if you used statsmodels it would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = sm.add_constant(np.log10(x))\n",
    "#print X\n",
    "#model = sm.OLS(np.log10(y), X, missing='drop').fit() # ignores entires where x or y is NaN\n",
    "#fit = model.fit()\n",
    "#return fit.params[1], fit.params[0], fit # could also return stderr in each via fit.bse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember to choose wisely which is your DEPENDENT and which is your INDEPENDENT variable. \n",
    "both for what you want to do logically (what changes as a functino of what) but also based on which  variable has a 'sensible' distribution that allows you to fit a function to it: \n",
    "    you need your independent variable to span a large range, and to be spread somewhat evenly in the range, \n",
    "    rather than just clustered in a spot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit and plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try making a fit for energy vs units AND a fit for units vs energy. \n",
    "tell me which is better. \n",
    "\n",
    "##look at the fit summary: which is better? could you do a likelihood ratio test here? why or why not??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#answer the question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##now fit a second degree curve to it: a*x^2+b*x+c. \n",
    "that is still called a linear fir by the way, the \"linear\" definition refers to the PARAMETERS being linear, not to the variable!\n",
    "you can use np.polyfit for example (an example showned in the other assignment notebook), with order 2 instead of 1,\n",
    "\n",
    "np.polyfit(x, y, 2)\n",
    "\n",
    "or statmodels.formula.api passing a formula like \n",
    "\n",
    "formula = 'y ~ 1 + x + I(x ^ 2.0)'\n",
    "\n",
    "##look at the fit summary: which is better? could you do a likelihood ratio test here? why or why not??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA CREDIT: \n",
    "    \n",
    "use WLS the weighted sq fit instead of OLS. you need to give weights to your points: \n",
    "estimates of the errors. the easiest thing is the sqrt of the datapoint value.\n",
    "if you want to be more sophisticated you can estimate the variance of your data\n",
    "near each point and use that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly: do it by hand and create a function that minimizes the residuals: \n",
    "       \n",
    "the residuals are the sum of the differences between data and model: \n",
    "in the case of a line fit model (and you can use your errors if you did the extra credit!)\n",
    "\n",
    "sum(np.log(bblnrgdatacut.nrg) - np.log(bblnrgdatacut.UnitsTotal)*a+b )^2\n",
    "\n",
    "where a and b are the parameters returned by the line fitter. \n",
    "for each data point you can calculate the model at different values : for example in a range B = np.arange (-100, 100, 1) for the intercept, and A = np.arange(-50.0, 50.0, 0.5) for the slope\n",
    "\n",
    "write it as a nested for loop (or challenge yourself and vectorize it!) with a loop inside another ranging all poissible combinations of the 2 variables (i use enumerate to get both an index from 0 to the size of my array, which i assign to i (and j) and the value of the array at that index - look up the syntax!):\n",
    "\n",
    "\n",
    "for i,a in enumerate(A):\n",
    "\n",
    "    for j,b in enumerate(B):\n",
    "    \n",
    "         Lsurface[i][j].....\n",
    "\n",
    "this gives you a 2D array that represents your likelihood surface! what we do to find a good fit is find the minimum (lowest point) on this surface. \n",
    "\n",
    "you can plot a surface (a 2D array) with pl.imshow(Lsurface) as a \"heatmap\"\n",
    "\n",
    "but when you do that you will find that the plot is very uninformative. just like you did before with the data, plot the log of it (pl.imshow(np.log(Lsurface))\n",
    "and see if it helps!\n",
    "also make sure your x and y axes tick numbers represent the range of values, not the cell index, which is the default for imshow. do that with the keyword extent: \n",
    "pl.imshow(Lsurface, .... , extent = [minimum, and max of your y array, minimum, and max of your x array]...) (notice that the numbers are switched! make sure to verify which axis is the slope, which is the intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##create the array\n",
    "##[...]\n",
    "##pl.figure(figsize=(10,10))\n",
    "##pl.title (\"log likelihood surface\", fontsize = 22)\n",
    "##pl.imshow(np.log(Lsurface), extent = ..., aspect=...)\n",
    "##[...]\n",
    "##pl.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA CREDIT: get creative with the dataset! can you make an insigntful plot to show any structure in the data?\n",
    "    \n",
    "below is my silly attempt at that\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bblnrgdatacut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7c895e25e602>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'YearBuilt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'YearBuilt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m1800\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1800\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scatter'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nrg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UnitsTotal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gist_rainbow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloglog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mxlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'YearBuilt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1900\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UnitsTotal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbblnrgdatacut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UnitsRes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bblnrgdatacut' is not defined"
     ]
    }
   ],
   "source": [
    "bblnrgdatacut['YearBuilt'][bblnrgdatacut['YearBuilt']<1800]=1800\n",
    "\n",
    "bblnrgdatacut.plot(kind='scatter',x='nrg',y='UnitsTotal', fontsize=22, colormap='gist_rainbow', alpha = 1, marker='o',  figsize=(16, 14), loglog=True,  xlim=(1000,1e11), ylim=(1,1000), c=bblnrgdatacut['YearBuilt']-1900, s=bblnrgdatacut['UnitsTotal']/bblnrgdatacut['UnitsRes']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
